# CS 4290

| Lectures                                                                |
| :---------------------------------------------------------------------- |
| [Introduction](#introduction)                                           |
| [Performance Metrics](#performance-metrics)                             |
| [Pipelining Review](#pipelining-review)                                 |
| [Branch Prediction and Predication](#branch-prediction-and-predication) |
| [Dependencies and ILP](#dependencies-and-ilp)                           |

---

## Introduction

### Ways to speed up a computer
1. speculation (just execute it)
2. prediction
3. parallelism

### Flynn's View of Parallelism
- SISD: single instruction, single data (single-threaded processors)
- SIMD: single instruction, multiple data (vector processors)
- MISD: multiple instruction, single data (i.e. cryptography)

### Hybrids
- SPMD - single program, multiple data (GPUs)

### Moore's Law

---

## Performance Metrics

### Choosing Metrics (Carefully)
- MIPS (million instructions per second)
- MFLOPS (million floating point operations per second)
- Peak FLOP - not representative (not always achievable)
- **run time (CPU time)** - does not include I/O

### CPU Performance
CPU time = CPU Clock Cycles * Clock cycle time = seconds per program

### RISC vs CISC

#### RISC - Simple Instructions (Reduced)
- increases instruction count (IC)
- easier to build hardware (decreases CT)
- more memory needed

#### CISC - Complex Instructions
- descreases IC
- easier to program
- harder to build fast hardware

CISC was used when memory was expensive, but now memory is cheap, and compilers are better, so RISC is more often used.

### Dealing with Numbers
- arithmetic mean to combine runtimes
- harmonic mean to combine rates
- geometric mean for normalized values 

### Amdahl's Law
speedup = old execution time / new execution time  
= 1 / ((1-fraction<sub>enhanced</sub>)+fraction<sub>enhanced</sub>/speedup<sub>enhanced</sub>))

- Make the common case faster (improve everything by a little bit, rather than highly optimizing a small part).
- diminishing returns - with each optimization of the same section, each optimization will have less effect

---

## Pipelining Review

### Typical Stages
- instruction fetch (IF)
- instruction decode (ID)
- execute (EX)
- memory (MEM)
- write-back (WB)

Most machines like this are not Von-Neuman?? machines, meaning that instructions are treated like data (they are both stored in memory).

### One instruction per cycle
- with no pipelining, the clock cycle time must be long enough for the most time consuming instruction
- with pipelining, the clock cycle time can be shortened to just the longest stage of the pipeline

### Dependencies
- RAW - true dependency

#### False Dependencies
Can be an issue in out of order processers
- WAR - anti-dependency
- WAW - 

### Register Files
- write on rising edge, and read on the falling edge (eliminates one noop from the pipeline with no data forwarding)

### Hazards
- data - dependencies prevent overlapped execution
- structural - not enough hardware resources (e.g. 3 adds in a row, but only 1 ALU - why is this an issue, since everything is being pipelined anyway??)
- control - branches-related stalls

---

## Branch Prediction and Predication

### Branch Stalls
- target address must be known one cycle after fetching branch, but the address is usually determined many slots later, even with direct branches (in the decode stage, one stage too late)

### Branch Delay Slots
- push problem to compiler, fill with instructions that are executed regardless of the branch
- too many cons (not feasible with longer pipelines)

### Branching
Execute anyway based on a guess, rather than waiting until the branch is resolved

| Type                                     | Whether                | Where       |
| ---------------------------------------- | ---------------------- | ----------- |
| Direct / Unconditional<br>Function Calls | Easy<br>(always taken) | Easy        |
| Conditional                              | Difficult              | Easy        |
| Indirect Jumps<br>Function Returns       | Easy<br>(always taken) | "Difficult" |

### Branch Target Buffer
- Holds the target of the last branch taken by the instruction at the given address
 (indexed by PC address).
- If an address matches, that branch is predicted to be taken.
- BTB entry is updated once the target is known

This is used to predict regular branches, and function calls (first two types in the table above).

### Function Returns
Use a return address stack.

#### Recursion
If the same function is called many times, the address does not continually need to be pushed (otherwise RAS will be filled, and previous addresses will be overwritten). Instead, count the number of times the same function has recursed, and only pop when reaching the end. Some extra hardware will be needed for this.

#### Full RAS
- overwrite, do nothing, or keep a counter

### Static Prediction
- always predict a certain direction

#### not taken
- easy to implement
- not so good accuracy (30-40%)

#### taken
- 60-70% accuracy

### Dynamic Prediction
- look up predictor table
- update branch history

### Use more than one bit
- create an FSM

### Branching Importance
- improving from 98% to 99% halves the number of mispredictions
- halving the miss rate doubles the number of useful instructions fetched

### Track the History of a Branch
- mix between FSM and keeping track of previous outcomes
- have a bit for past outcome, and counters for each state of the previous outcome
- n bits for last n outcomes, 2^n counters

#### Local History
- using the previously mentioned techniques
- predicting the direction of Branch A given the previous outcomes of instances of Vranch A

#### Global History
- predicted direction of a branch Z given outcomes of all previous branches (not limited to only branch Z)
- limited by the history length
- this is useful for related branch conditions (e.g. `if` statements with opposite clauses)

### Bimodal Predictor
- generate pattern history table (PHT) index by hashing the branch PC value
- each entry in the PHT is a counter for the given PC value

### G-select
- generate PHT index by concatenating PC with global history
- usually equal numbers of bits from PC and history (n/2)

### G-share
- concatenation in G-select will waste a lot of rows
- instead, generate the PHT index by xor-ing branch PC with global history

### Local History Predictor
- two layer indexing; one history register table (HRT) to record history for each PC, and a separate PHT for each history entry
- no one uses this anymore

### Two-Level Adaptive Predictor
- instead of separate PHTs for each entry, have a single global PHT
- use PC to index into HRT, and HRT entry value to index into PHT

### Loop Predictor
- previous predictors are not very useful for loops
- have a separate loop predictor
- how to determine if a branch is a loop?

### Tournament Predictor
- use a predictor to determine which predictor will be better
- "meta-predictor" will select which predictor to use for a given branch

### Updating History
- branches can occur one after another (nested ifs, for-loops, etc.)
- for deeper pipelines, waiting for the outcome of the branch (the EXECUTE stage) may take too long
- option: use predicted value to update history

### Predication
- if-else code often is very short
- don't try to guess, just do both sides
- how to keep only the desired results?

### CMOV
- `CMOV R1, R2, R3`
- R1 = R3 ? R2: R1
- not good; too many registers (CMOV needed for each value)

### "Full" Predication
- every instruction is predicated
- separate predicate registers and separate instructions to set predicates
- do everything, and save the results based on the condition (every stage before MEM and WB is always executed)
- extra bits needed in every instruction
- must change ISA

### Exceptions
- when a predicate is false, and the nested instruction causes an exception, the exception must not be shown
- instruction must appear as a no-op

### if-conversion

#### benefits
- becomes branch-free code
- potential performance improvement??
- more scheduling flexibility

#### downsides
- guaranteed resource waste
- limited to small sequences

### Predict or Predicate?
usually we predict, but it depends

---

## Dependencies and ILP

### Instruction Level Parallelism (ILP)
- execute several instructions in parallel
- pipelining will only push through at most 1 instruction per cycle

#### Scope of ILP
- this is on a single thread level
- program level parallelism is the responsibility of the OS (executing different threads in parallel)
- running *sequential* code

#### "Correct" Excecution
processor state (registers, PC, memory) should be as if instructions were executed one at a time

### Superscalar and Vector CPUs
- "scalar" - executes one instruction at a time
- "vector" - one instruction at a time, but on vector data (e.g. a single register represents multiple data / dividing bits)
- "superscalar" - can execute more than one unrelated instruction at a time

### Modeling Stalls
- speedup = CPI<sub>no pipe</sub> / CPI<sub>pipe</sub>
- s = average number of stalls per instruction
- CPI<sub>pipe</sub> = CPI<sub>no stall</sub> (= 1) + s
- speedup = number of stages / (1 + s)

### False Dependencies
- a finite number of registers means overwriting is going to occur
- WAR and WAW are also called "name dependencies"; they come from reusing registers

#### Adding More Registers
- cannot be done because that would mean changing the ISA, and thus there is no more backwards compatibility (all previous code cannot be reused)
- does not address register overwriting from code reuse in loops and function calls

#### Register Renaming
- add more registers, but only in hardware (don't expose it to the ISA)
- temporarily map ISA registers to physical registers