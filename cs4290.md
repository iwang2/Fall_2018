# CS 4290 - Advanced Computer Organization

| Lectures                                                                                          |
| :-----------------------------------------------------------------------------------------------: |
| [Introduction](#introduction)                                                                     |
| [Performance Metrics](#performance-metrics)                                                       |
| [Pipelining Review](#pipelining-review)                                                           |
| [Branch Prediction and Predication](#branch-prediction-and-predication)                           |
| [Dependencies and ILP](#dependencies-and-ilp)                                                     |
| [Dynamic Instruction Scheduling](#dynamic-instruction-scheduling)                                 |
| [Interrupts, Exceptions, and Memory Dependencies](#interrupts-exceptions-and-memory-dependencies) |
| [Compiler ILP Techniques](#compiler-ilp-techniques)                                               |
| [Virtual Memory and Memory Protection](#virtual-memory-and-memory-protection)                     |
| [Caches](#caches)                                                                                 |

---

## Introduction

### Ways to speed up a computer
1. speculation (just execute it)
2. prediction
3. parallelism

### Flynn's View of Parallelism
- SISD: single instruction, single data (single-threaded processors)
- SIMD: single instruction, multiple data (vector processors)
- MISD: multiple instruction, single data (i.e. cryptography)

### Hybrids
- SPMD - single program, multiple data (GPUs)

### Moore's Law

---

## Performance Metrics

### Choosing Metrics (Carefully)
- MIPS (million instructions per second)
- MFLOPS (million floating point operations per second)
- Peak FLOP - not representative (not always achievable)
- **run time (CPU time)** - does not include I/O

### CPU Performance
CPU time = CPU Clock Cycles * Clock cycle time = seconds per program

### RISC vs CISC

#### RISC - Simple Instructions (Reduced)
- increases instruction count (IC)
- easier to build hardware (decreases CT)
- more memory needed

#### CISC - Complex Instructions
- descreases IC
- easier to program
- harder to build fast hardware

CISC was used when memory was expensive, but now memory is cheap, and compilers are better, so RISC is more often used.

### Dealing with Numbers
- arithmetic mean to combine runtimes
- harmonic mean to combine rates
- geometric mean for normalized values 

### Amdahl's Law
speedup = old execution time / new execution time  
= 1 / ((1-fraction<sub>enhanced</sub>)+fraction<sub>enhanced</sub>/speedup<sub>enhanced</sub>))

- Make the common case faster (improve everything by a little bit, rather than highly optimizing a small part).
- diminishing returns - with each optimization of the same section, each optimization will have less effect

---

## Pipelining Review

### Typical Stages
- instruction fetch (IF)
- instruction decode (ID)
- execute (EX)
- memory (MEM)
- write-back (WB)

Most machines like this are not Von-Neuman?? machines, meaning that instructions are treated like data (they are both stored in memory).

### One instruction per cycle
- with no pipelining, the clock cycle time must be long enough for the most time consuming instruction
- with pipelining, the clock cycle time can be shortened to just the longest stage of the pipeline

### Dependencies
- RAW - true dependency

#### False Dependencies
Can be an issue in out of order processers
- WAR - anti-dependency
- WAW - 

### Register Files
- write on rising edge, and read on the falling edge (eliminates one noop from the pipeline with no data forwarding)

### Hazards
- data - dependencies prevent overlapped execution
- structural - not enough hardware resources (e.g. 3 adds in a row, but only 1 ALU - why is this an issue, since everything is being pipelined anyway??)
- control - branches-related stalls

---

## Branch Prediction and Predication

### Branch Stalls
- target address must be known one cycle after fetching branch, but the address is usually determined many slots later, even with direct branches (in the decode stage, one stage too late)

### Branch Delay Slots
- push problem to compiler, fill with instructions that are executed regardless of the branch
- too many cons (not feasible with longer pipelines)

### Branching
Execute anyway based on a guess, rather than waiting until the branch is resolved

| Type                                     | Whether                | Where       |
| ---------------------------------------- | ---------------------- | ----------- |
| Direct / Unconditional<br>Function Calls | Easy<br>(always taken) | Easy        |
| Conditional                              | Difficult              | Easy        |
| Indirect Jumps<br>Function Returns       | Easy<br>(always taken) | "Difficult" |

### Branch Target Buffer
- Holds the target of the last branch taken by the instruction at the given address
 (indexed by PC address).
- If an address matches, that branch is predicted to be taken.
- BTB entry is updated once the target is known

This is used to predict regular branches, and function calls (first two types in the table above).

### Function Returns
Use a return address stack.

#### Recursion
If the same function is called many times, the address does not continually need to be pushed (otherwise RAS will be filled, and previous addresses will be overwritten). Instead, count the number of times the same function has recursed, and only pop when reaching the end. Some extra hardware will be needed for this.

#### Full RAS
- overwrite, do nothing, or keep a counter

### Static Prediction
- always predict a certain direction

#### not taken
- easy to implement
- not so good accuracy (30-40%)

#### taken
- 60-70% accuracy

### Dynamic Prediction
- look up predictor table
- update branch history

### Use more than one bit
- create an FSM

### Branching Importance
- improving from 98% to 99% halves the number of mispredictions
- halving the miss rate doubles the number of useful instructions fetched

### Track the History of a Branch
- mix between FSM and keeping track of previous outcomes
- have a bit for past outcome, and counters for each state of the previous outcome
- n bits for last n outcomes, 2^n counters

#### Local History
- using the previously mentioned techniques
- predicting the direction of Branch A given the previous outcomes of instances of Vranch A

#### Global History
- predicted direction of a branch Z given outcomes of all previous branches (not limited to only branch Z)
- limited by the history length
- this is useful for related branch conditions (e.g. `if` statements with opposite clauses)

### Bimodal Predictor
- generate pattern history table (PHT) index by hashing the branch PC value
- each entry in the PHT is a counter for the given PC value

### G-select
- generate PHT index by concatenating PC with global history
- usually equal numbers of bits from PC and history (n/2)

### G-share
- concatenation in G-select will waste a lot of rows
- instead, generate the PHT index by xor-ing branch PC with global history

### Local History Predictor
- two layer indexing; one history register table (HRT) to record history for each PC, and a separate PHT for each history entry
- no one uses this anymore

### Two-Level Adaptive Predictor
- instead of separate PHTs for each entry, have a single global PHT
- use PC to index into HRT, and HRT entry value to index into PHT

### Loop Predictor
- previous predictors are not very useful for loops
- have a separate loop predictor
- how to determine if a branch is a loop?

### Tournament Predictor
- use a predictor to determine which predictor will be better
- "meta-predictor" will select which predictor to use for a given branch

### Updating History
- branches can occur one after another (nested ifs, for-loops, etc.)
- for deeper pipelines, waiting for the outcome of the branch (the EXECUTE stage) may take too long
- option: use predicted value to update history

### Predication
- if-else code often is very short
- don't try to guess, just do both sides
- how to keep only the desired results?

### CMOV
- `CMOV R1, R2, R3`
- R1 = R3 ? R2: R1
- not good; too many registers (CMOV needed for each value)

### "Full" Predication
- every instruction is predicated
- separate predicate registers and separate instructions to set predicates
- do everything, and save the results based on the condition (every stage before MEM and WB is always executed)
- extra bits needed in every instruction
- must change ISA

### Exceptions
- when a predicate is false, and the nested instruction causes an exception, the exception must not be shown
- instruction must appear as a no-op

### if-conversion

#### benefits
- becomes branch-free code
- potential performance improvement??
- more scheduling flexibility

#### downsides
- guaranteed resource waste
- limited to small sequences

### Predict or Predicate?
usually we predict, but it depends

---

## Dependencies and ILP

### Instruction Level Parallelism (ILP)
- execute several instructions in parallel
- pipelining will only push through at most 1 instruction per cycle

#### Scope of ILP
- this is on a single thread level
- program level parallelism is the responsibility of the OS (executing different threads in parallel)
- running *sequential* code

#### "Correct" Excecution
processor state (registers, PC, memory) should be as if instructions were executed one at a time

### Superscalar and Vector CPUs
- "scalar" - executes one instruction at a time
- "vector" - one instruction at a time, but on vector data (e.g. a single register represents multiple data / dividing bits)
- "superscalar" - can execute more than one unrelated instruction at a time

### Modeling Stalls
- speedup = CPI<sub>no pipe</sub> / CPI<sub>pipe</sub>
- s = average number of stalls per instruction
- CPI<sub>pipe</sub> = CPI<sub>no stall</sub> (= 1) + s
- speedup = number of stages / (1 + s)

### False Dependencies
- a finite number of registers means overwriting is going to occur
- WAR and WAW are also called "name dependencies"; they come from reusing registers

#### Adding More Registers
- cannot be done because that would mean changing the ISA, and thus there is no more backwards compatibility (all previous code cannot be reused)
- does not address register overwriting from code reuse in loops and function calls

#### Register Renaming
- add more registers, but only in hardware (don't expose it to the ISA)
- temporarily map ISA registers to physical registers
- ARF (Architectural Register File)
- RAT (Register Alias Table) - holds a pointer to the appropriate register (either to the ARF, or the PRF if the register is being aliased)
- PRF (Physical Register File)

#### Register Reuse
- re-use can occur once the instruction that writes to the register retires from the pipeline
- however, the RAT cannot always be updated to point back to the ARF

### ILP
- number of instructions / longest path

### Dynamic (Out-of-Order) Scheduling
- window size - how many instructions ahead to look
- im tired of taking notes

---

## Dynamic Instruction Scheduling

### Scoreboarding
- do everything sequentially
- no register renaming
- detects WAR and WAW, but cannot eliminate (stalls)

#### Stages
- fetch - bring instructions from memory
- issue - decode instructions, and check for dependencies (stall if there are)
- read operands - wait until all of them are available
- execute - after the result is ready, the scoreboard is notified
- write result - delayed until earlier instructions that intend to read have completed their read operands stage

#### Data Structures
- instruction status - indicates which stage each instruction is in
- function unit status - indicates the state of each (9 fields)
- register status - which functional unit will write to the given register

### Tomasulo's Algorithm
- removes name dependencies through register renaming

#### Hardware
- instruction buffers
- register file, RAT (0 if copy from register file, 1 if waiting on result from a functional unit)
- 1 adder (3 slots), 1 Mul/Div (2 slots) - reservation stations

#### Issue
- get next instruction
- find a free reservation station (or stall if none are available)
- read operands from registers (or find the reservation station that will produce it)
- renames registers (reservation station IDs)

#### Execute
- monitor results as they are produced
- broadcast them to the reservation stations that are waiting on the given result; done by the CDB (common data bus)
- when more than 1 instruction is ready from the same unit, execute by oldest first

#### Write
- used CDB to make the result available (writeback to both register file and waiting reservation stations)
- update the register mapping (only if the RAT contains your mapping)
- free the reservation station

#### Load/Store
- reservation stations take care of register dependencies
- however, memory operations are not

#### Order of Execution
- in-order: issue and decode
- out-or-order: execute, writeback

### Problems with Tomasulo
- a lot, but basically you should separate register renaming from the reservation stations
- instructions are completed out of order, which makes exceptions and branch mispredication difficult to recover from (debugging is more difficult)

### Re-Order Buffer (ROB)
- separate architectural and physical registers
- allows for in-order completion

#### Hardware
- instruction buffers
- RAT (alias), which may point to ART or ROB
- ALUs and reservation stations (which will no longer point to other reservation stations, but the ROB instead)

#### Issue
- read instruction from buffer
- check for resources (appropriate RS and ROB entries, stall otherwise)

#### Execute
- same as before

#### Write Result
- broadcast result on CDB
- write result back to ROB entry only; the ARF will be updated in program order
- reservation station can be freed (or sooner, since the ROB keeps track)

#### Commit (new stage)
- occurs when an instruction is the oldest in the ROB
- make instruction execution "visible"

#### Register Renaming
- reservation stations can be freed earlier now (apply the same logic to the ROB)
- like before, update the alias only if it is pointing to the given entry

### Unified Reservation Stations
- instead of having separate reservation stations per functional unit, combine reservation stations to be able to hold values for either
- makes hardware a little more complex 

### Actual Superscalar Execution
- previous example can only reach a maximum of 1 IPC (only 1 CDB, superscalar fetch, decode, etc. needed)
- each stage must be able to handle more than 1 instruction at a time (Tomasulo can already do this for the execute stage, given that there are available functional units and instructions)

### Dual-Issue (Two-Way Superscalar)
- check resource availability for two instructions (RS and ROB entries)
- read for operands - most complicated (RAT/ARF/ROB + renaming)
- upate RS/ROB entries

#### Dual Rename
When there are no RAW dependencies, renaming is easy.
- use the current mappings in the RAT for each operand
- new destinations are just given the next two ROB entries (allocate `ROB_tail` and `ROB_tail + 1`)
```
    RAT         Original Instructions:    After Renaming:
    _______
R1 |_ARF1__|    R1 = R2 + R3              ROB21 = ROB17 + ARF3
R2 |_ROB17_|    R4 = R2 - R4              ROB22 = ROB17 - ROB6
R3 |_ARF3__|
R4 |_ROB6__|
```

However, when there are RAW dependencies, the operand that depends on a write value cannot simply use the given mapping in the RAT.
```
    RAT         Original Instructions:    After Renaming:
    _______
R1 |_ARF1__|    R1 = R2 + R3              ROB21 = ROB17 + ARF3
R2 |_ROB17_|    R4 = R1 - R4              ROB22 = ROB21 - ROB6
R3 |_ARF3__|                                       ^^
R4 |_ROB6__|                                    not ARF1
```

#### Multiple CDBs
- with multiple CDBs, each reservation station entry must be checked multiple times (once for each CDB) to verify whether it matches with the source of the CDB value
- this requires more area, logic, power ($$$)

#### Committting more than 1 IPC
Must check which instructions are ready to commit
| Head     | Next     | What Gets Committed |
| -------- | -------- | ------------------- |
| not done | -        | nothing             |
| done     | not done | head                |
| done     | done     | head and next       |

Must be able to write-back multiple results at once from ROB to ARF (or memory for stores)
- extra read ports for ROB, write ports for ARF

### Ordering in Each Stage
```
FETCH | DECODE | ISSUE     EXECUTE | WRITE      COMMIT

      in-order              out of order       in-order
```

---
## Interrupts, Exceptions, and Memory Dependencies

### Interrupts vs. Exceptions

#### Interrupts
- caused by external events
- asynchronous to program execution (unrelated to the program)
- simply suspend and resume user program

#### Exceptions
- caused by internal events
- synchronous to program execution
- system takes action to handle (handler)
- instruction may be retired and program continued, or the program is aborted

### Exception Generating Stages
| Stage   | Exceptions                                                                             |
| ------- | -------------------------------------------------------------------------------------- |
| fetch   | page fault on instruction fetch, misaligned memory access, memory-protection violation |
| decode  | undefined or illegal opcode                                                            |
| execute | arithmetic exception                                                                   |
| memory  | similar to fetch                                                                       |

### Exceptions Should be Precise
The state of the machine is preserved as if program executed up to the exception-causing instruction
- prior instructions are executed
- subsequent instruction did not modify process state
- exception causing instruction may or may not have been executed (depends on architecture and exception)

#### Treat Faults Like a Result
- at execution, make note of faults
- if the instruction gets to commit, *then* expose the fault

### Branch Misprediction
- must undo wrong-path register changes

#### Checkpointing
- at each branch, make a copy of the RAT

##### On Misprediction
1. mark wrong-path instructions in ROB
2. deallocate wrong-path RAT checkpoints
3. recover RAT from checkpoint
4. start fetching the correct instructions and resume

### Memory Dependencies
- only write stores during commit

#### Memory Disambiguation
- are there earlier unexcuted stores to the same address as the current load?

#### Store-to-load forwarding
- load: which earlier store to get value from?
- store: which later load(s) to transmit value to?

### Load Store Queue (LSQ)
- similar to the ROB
- loads cannot execute until all earlier store addresses are computed
- ST puts the value in the LSQ (for forwarding and commits)
- LD puts the value in the ROB

#### Issue
- allocate LSQ entry for each LD/ST (in addition to ROB entry, RS, etc.)

#### Execute
- generate address (update LSQ)
- produce result (update ROB)

#### Commit
- if ST, send value to memory
- move LSQ head

---
## Compiler ILP Techniques

Dependence chains limit ILP, and its hard to continue to modify hardware to improve it (ROB, RS limits).

### Tree Height Reduction
Shorten using associativity
```
   ADD R6, R2, R3          ADD R6, R2, R3
   ADD R7, R6, R4          ADD R7, R4, R5
   ADD R8, R7, R5          ADD R8, R7, R6

         I1
         |                     I1  I2
         I2                     \  /
         |                       I3
         I3

R8 = ((R3+R3)+R4)+R5    R8 = (R2+R3)+(R4+R5)
```

### Simple Loops
```c
for (i = 1000; i > 0; i--) {
    x[i] = x[i] + s;
}
```

#### Assume
- single-issue pipeline
- add to store: +2 cycles
- load to add: +1 cycle
- branch: +1 cycle

```
Loop:   LD  F0, 0(R1)                   Loop:   LD  F0, 0(R1)
        - stall                                 ADD R1, R1 #-8
        ADD F0, F0, F2                          ADD F0, F0, F2
        - stall            -------->            - stall
        - stall              hoist              - stall
        SD  F0, 0(R1)       the add        ---> SD  F0, 8(R1)
        ADD R1, R1, #-8                   |     - stall
        - stall                        offset   BNE R1, R2, Loop
        BNE R1, R2, Loop              modified
```

### Scheduling

#### Branches
Suppose a branch can go two ways, and one outcome requires a load. If waiting for the branch outcome takes many cycles, one option is to preemptively execute the load to before the branch. However, this would cause unnecessary execution when the branch goes in the other direction (aka **Dynamic Dead Code**).

#### Predication
Predication may help, but not when there is no "common case" (branch is 50-50).

Trying hardware stuff at the compiler level.

### Loop Unrolling
Transform an M-iteration loop into a M/N iteration loop.

```c
for (i = 1000; i > 0; i--) {
    x[i] = x[i] + s;
}
```

Unrolled:
```c
for (i = 1000; i > 0; i -= 4) {
    x[i] = x[i] + s;
    x[i+1] = x[i+1] + s;
    x[i+2] = x[i+2] + s;
    x[i+3] = x[i+3] + s;
}
```

#### Benefits
- fewer branches
- better scheduling of instructions (more instructions to choose from when reordering)
- get rid of small loops

#### Problems
- more code (more complicated to understand)
- What if N is not a multiple of M? (extra code)
- What if N is not known at compile time?
- while loops?

```c
for (i = 0; i < j; i++>) {
    a[i] *= 2;
}
```

### Function Inlining
- like "unrolling" a function (copy function code directly instead of calling the function)
- remove function call overhead
- larger block of instructions for scheduling

#### Similar Problems
- increase register pressure
- extra code
- recursive functions?

### Software Pipelining
Unrolling is not always enough.
```c
for (i = 0; i < 100; i++) {
    sum += a[i] * b[i];
}
```
#### Create Stages
- load, multiply, sum
```c
for (i = 0; i < 100; i++) {
    // Stage 1
    ai = a[i];
    bi = b[i];
    // Stage 2
    prod = ai * bi;
    // Stage 3
    sum += prod;
}
```

#### Pipelining

The code in the slides makes no sense?????

This is basically just getting rid of RAW dependencies.

```c
// setup variables
prod = a[0] * b[0];
a = a[1], b = b[1];

// the last product to be added from the loop is a[97] * b[97]
for (i = 2; i < 100; i++) {
    sum += prod;         // sum += a[i-2] * b[i-2]
    prod = a * b;        // prod = a[i-1] * b[i-1]
    a = a[i]; b = b[i];  // current iteration
}

// finish up the last two iterations
sum += prod;  // sum += a[98] * b[98]
sum += a * b; // sum += a[99] * b[99]
```

#### Problems
- more code
- register pressure
- must know exit condition
- works only for loops

---
## Virtual Memory and Memory Protection

you learned this already
- programmer is unconcerned about memory (virtual > physical)
- memory is divided into pages
- page tables to map (w/ permissions)

### Page Tables
- single page table takes up a lot of space
- use multi-level page tables instead
- more space is needed for multiple levels, but space is saved because not all entries need to be allocated (thus removing entire tables)

#### Page Size
- small pages: bigger page table, but less fragmentation
- big pages: efficient transfer to/from disk

### CPU Memory Access
1. compute virtual address (adding offset)
2. compute virtual page number
3. compute physical address of VPN's page table entry
4. load mapping (for each table level)
5. compute physical address
6. do the actual load from memory

#### Performance Impact
- each load/store requires at least 2 accesses
- each fetch requires translation
- however, once a virtual page is mapped, it will most likely remain for awhile

### TLB (Translation Look-Aside Buffer)
- caching translations to avoid the performance impact

What happens when there is a TLB miss?
- hardware (fast) - register to keep table location, and hardware can read the page table (stall until translation is found)
- software (slow) - raise an exception, use a miss handler in the OS, and then retry the instruction

#### TLB Design

##### Fully Associative (old)
- any mapping can be kept in any entry
- check all entries on each access
- few entries to maintain low latency
- how big should the working set be to avoid TLB misses?
- if there are many misses: increase TLB size (latency problems), increase page size

##### Set-Associative (current)
- programs are getting bigger, so the TLB must remember more mappings
- mapping goes to a specific part of the TLB using lowest VPN bits
- some use multi-level TLBs

#### The TLB is not a Cache
- the TLB saves only the final translation
- a cache can save the middle layers (the mapping between page tables)

#### Process Changes
- the TLB is invalid after a context switch
- instead of flushing the entire TLB, add a PID to the translation to check validity
- only flush when recycling PIDs

### Memory Protection
- permission bits for each page, kept in page table and TLB entry, checked during translation

#### OS Considerations
- system mode: access to physical memory, and allows execution of special instructions
- user mode: can't do those things

---
## Caches

### Locality
- **temporal** - if data item needed now, likely needed again in near future
- **spatial** - if data item needed now, nearby data likely needed in near future
- caches - keep recently used data in fast memory close to processor (and bring nearby data)

|             Storage Hierarchy             |
| :---------------------------------------: |
|                   Disk                    |
|                Main Memory                |
|                 L3 Cache                  |
|                 L2 Cache                  |
| ITLB, Instruction Cache, Data Cache, DTLB |
|               Register File               |
|              Bypass Network               |
- from high capacity, low speed, to low capacity, high speed

### Cache Basics
- you know this already
- if something is not in the cache, bring in the entire block of data

#### Important Decisions
- placement: where does it go?
- identification: how to find a block in the cache?
- replacement: what gets kicked out to make room?
- write policy: how to handle stores?

#### Structure
- block-sized lines
- blocks are typically 16-128 bytes in size (most commonly 64)
- use address offset to index within block

### Cache Placement
How to assign memory blocks to cache lines
- direct mapped (each block can go to one line only)
- fully associative (any block can go to any line)
- set-associative (block can go to one of N lines)

#### Direct Mapped
Each block can go to one line only.
```
           ______________ _______ ________________
Address   |_____Tag______|_Index_|_____Offset_____|
                 |           |            |
              hit or     select bits     block
                miss      for cache      size
```
Tags are really expensive in hardware, there are a lot of bits to compare.

#### Fully Associative
```
           ______________________ ________________
Address   |__________Tag_________|_____Offset_____|
```
The block can go anywhere in the cache (there is no more index), so each line in the cache must be checked (larger tag as well).

#### N-Way Set Associative
- N represents the size of each set
```
           ______________ _______ ________________
Address   |_____Tag______|_Index_|_____Offset_____|
                             |
         _____            to index
        |  0  | set 0     into set
        |__1__|          (2 bits for
Cache   |  2  | set 1      4 sets)
Line #  |__3__|
        |  4  | set 2
        |__5__|
        |  6  | set 3
        |__7__|
```
- 1-way set associative = direct mapped
- N-way set associative = fully associative

### Cache Identification
- cache lookup - find whether data is in the cache, and where
- each cache line has a **valid** bit, and a **tag** to identify which block is in the line

### Examples
1. **Direct mapped, 32-byte line, 256 bytes total (cache size)**
   - block offset is 5 bits (32-byte line)
   - 8 lines (256/32)
   - index is 3 bits (for 8 lines)
   - rest of the address is tag bits (8..63)

    When accessing the cache:
    - use index bits to find the line
    - use tag bits to check if they match
    - if no match, fetch block from memory
    - use offset bits to get correct block in line

2. **Fully-associative, 16-byte line, 1024 bytes total**
   - block offset is 4 bits (from 16-byte line)
   - 64 lines (1024/16)
   - no cache index, so bits 4..63 are tag

   When accessing the cache:
   - use tag bits to check all lines for a match

3. **8-way set associative, 64-byte line, 2KB total**
   - 6 bit block offset (64 byte line)
   - 32 lines (2048/64)
   - 4 sets (32 lines / 8 lines per set)
   - 2 bit index (for 4 sets)
   
   When accessing the cache:
   - use index bits to identify which set
   - use tag bits to find if any lines within the set match

### Cache Replacement
Which block gets kicked out when a free line is needed?
- random
- FIFO (line that has been in the cache the longest)
- **LRU (least recently used)**
- LRU Approximations
- NMRU (not most recently used)
- LFU (least frequently used)

#### Implementing LRU
- have LRU counter for each line in a set

When a line is accessed:
- get old value of its counter
- set counter to max value
- for every other line in the set greater than the old value, decrement by 1

When a line is needed, select the one with the counter at 0.

### Write Policy
Do we allocate cache lines on a write?
- write-allocate - write miss brings block into cache
- no-write-allocate - write miss leaves cache alone

Do we update memory on writes?
- write-through - memory immediately updated on each write
- write-back - memory updated when line is replaced

Write-allocate + write-back vs. no-write allocate + write-through

#### Write-Back Caches
- dirty bit for each line (dirty if line has been written to and memory has not been updated)
- replacing a dirty line - must write-back to memory

### TLB Interaction
- virtual vs. physical to access the cache?

#### PAPT (Physically-Addressed Physically-Tagged)
- translate the virtual address, and use the physical address to index into the cache
- slow, bc translation may be slow

#### Virtually Addressed
- fast; no need to check TLB
- must flush cache on process change (very costly)
- hard to enforce permissions (TLB isn't being checked)
- aliasing problem - several virtual pages can map to the same physical page

#### Virtually Indexed Physically Tagged
Index into cache with the virtual address, while translating simultaneously to get the physical address. Then, compare the physical address to the tag from the cache.
- fast; TLB parallelized with caching
- no need to flush the cache on a process swap
- aliasing is still an issue