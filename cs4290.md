# CS 4290 - Advanced Computer Organization

|                                             Lectures                                              |
| :-----------------------------------------------------------------------------------------------: |
|                                   [Introduction](#introduction)                                   |
|                            [Performance Metrics](#performance-metrics)                            |
|                              [Pipelining Review](#pipelining-review)                              |
|              [Branch Prediction and Predication](#branch-prediction-and-predication)              |
|                           [Dependencies and ILP](#dependencies-and-ilp)                           |
|                 [Dynamic Instruction Scheduling](#dynamic-instruction-scheduling)                 |
| [Interrupts, Exceptions, and Memory Dependencies](#interrupts-exceptions-and-memory-dependencies) |
|                        [Compiler ILP Techniques](#compiler-ilp-techniques)                        |
|           [Virtual Memory and Memory Protection](#virtual-memory-and-memory-protection)           |
|                                         [Caches](#caches)                                         |
|                              [Cache Performance](#cache-performance)                              |
|                                    [Prefetching](#prefetching)                                    |
|                                         [Memory](#memory)                                         |
|                                    [Reliability](#reliability)                                    |
|             [Multiprocessing and Multithreading](#multiprocessing-and-multithreading)             |
|                                [Cache Coherence](#cache-coherence)                                |
|                [Directory-Based Cache Coherence](#directory-based-cache-coherence)                |

---

## Introduction

### Ways to speed up a computer
1. speculation (just execute it)
2. prediction
3. parallelism

### Flynn's View of Parallelism
- SISD: single instruction, single data (single-threaded processors)
- SIMD: single instruction, multiple data (vector processors)
- MISD: multiple instruction, single data (i.e. cryptography)

### Hybrids
- SPMD - single program, multiple data (GPUs)

### Moore's Law

---

## Performance Metrics

### Choosing Metrics (Carefully)
- MIPS (million instructions per second)
- MFLOPS (million floating point operations per second)
- Peak FLOP - not representative (not always achievable)
- **run time (CPU time)** - does not include I/O

### CPU Performance
CPU time = CPU Clock Cycles * Clock cycle time = seconds per program

### RISC vs CISC

#### RISC - Simple Instructions (Reduced)
- increases instruction count (IC)
- easier to build hardware (decreases CT)
- more memory needed

#### CISC - Complex Instructions
- descreases IC
- easier to program
- harder to build fast hardware

CISC was used when memory was expensive, but now memory is cheap, and compilers are better, so RISC is more often used.

### Dealing with Numbers
- arithmetic mean to combine runtimes
- harmonic mean to combine rates
- geometric mean for normalized values 

### Amdahl's Law
speedup = old execution time / new execution time  
= 1 / ((1-fraction<sub>enhanced</sub>)+fraction<sub>enhanced</sub>/speedup<sub>enhanced</sub>))

- Make the common case faster (improve everything by a little bit, rather than highly optimizing a small part).
- diminishing returns - with each optimization of the same section, each optimization will have less effect

---

## Pipelining Review

### Typical Stages
- instruction fetch (IF)
- instruction decode (ID)
- execute (EX)
- memory (MEM)
- write-back (WB)

Most machines like this are not Von-Neuman?? machines, meaning that instructions are treated like data (they are both stored in memory).

### One instruction per cycle
- with no pipelining, the clock cycle time must be long enough for the most time consuming instruction
- with pipelining, the clock cycle time can be shortened to just the longest stage of the pipeline

### Dependencies
- RAW - true dependency

#### False Dependencies
Can be an issue in out of order processers
- WAR - anti-dependency
- WAW - 

### Register Files
- write on rising edge, and read on the falling edge (eliminates one noop from the pipeline with no data forwarding)

### Hazards
- data - dependencies prevent overlapped execution
- structural - not enough hardware resources (e.g. 3 adds in a row, but only 1 ALU - why is this an issue, since everything is being pipelined anyway??)
- control - branches-related stalls

---

## Branch Prediction and Predication

### Branch Stalls
- target address must be known one cycle after fetching branch, but the address is usually determined many slots later, even with direct branches (in the decode stage, one stage too late)

### Branch Delay Slots
- push problem to compiler, fill with instructions that are executed regardless of the branch
- too many cons (not feasible with longer pipelines)

### Branching
Execute anyway based on a guess, rather than waiting until the branch is resolved

| Type                                     | Whether                | Where       |
| ---------------------------------------- | ---------------------- | ----------- |
| Direct / Unconditional<br>Function Calls | Easy<br>(always taken) | Easy        |
| Conditional                              | Difficult              | Easy        |
| Indirect Jumps<br>Function Returns       | Easy<br>(always taken) | "Difficult" |

### Branch Target Buffer
- Holds the target of the last branch taken by the instruction at the given address
 (indexed by PC address).
- If an address matches, that branch is predicted to be taken.
- BTB entry is updated once the target is known

This is used to predict regular branches, and function calls (first two types in the table above).

### Function Returns
Use a return address stack.

#### Recursion
If the same function is called many times, the address does not continually need to be pushed (otherwise RAS will be filled, and previous addresses will be overwritten). Instead, count the number of times the same function has recursed, and only pop when reaching the end. Some extra hardware will be needed for this.

#### Full RAS
- overwrite, do nothing, or keep a counter

### Static Prediction
- always predict a certain direction

#### not taken
- easy to implement
- not so good accuracy (30-40%)

#### taken
- 60-70% accuracy

### Dynamic Prediction
- look up predictor table
- update branch history

### Use more than one bit
- create an FSM

### Branching Importance
- improving from 98% to 99% halves the number of mispredictions
- halving the miss rate doubles the number of useful instructions fetched

### Track the History of a Branch
- mix between FSM and keeping track of previous outcomes
- have a bit for past outcome, and counters for each state of the previous outcome
- n bits for last n outcomes, 2^n counters

#### Local History
- using the previously mentioned techniques
- predicting the direction of Branch A given the previous outcomes of instances of Vranch A

#### Global History
- predicted direction of a branch Z given outcomes of all previous branches (not limited to only branch Z)
- limited by the history length
- this is useful for related branch conditions (e.g. `if` statements with opposite clauses)

### Bimodal Predictor
- generate pattern history table (PHT) index by hashing the branch PC value
- each entry in the PHT is a counter for the given PC value

### G-select
- generate PHT index by concatenating PC with global history
- usually equal numbers of bits from PC and history (n/2)

### G-share
- concatenation in G-select will waste a lot of rows
- instead, generate the PHT index by xor-ing branch PC with global history

### Local History Predictor
- two layer indexing; one history register table (HRT) to record history for each PC, and a separate PHT for each history entry
- no one uses this anymore

### Two-Level Adaptive Predictor
- instead of separate PHTs for each entry, have a single global PHT
- use PC to index into HRT, and HRT entry value to index into PHT

### Loop Predictor
- previous predictors are not very useful for loops
- have a separate loop predictor
- how to determine if a branch is a loop?

### Tournament Predictor
- use a predictor to determine which predictor will be better
- "meta-predictor" will select which predictor to use for a given branch

### Updating History
- branches can occur one after another (nested ifs, for-loops, etc.)
- for deeper pipelines, waiting for the outcome of the branch (the EXECUTE stage) may take too long
- option: use predicted value to update history

### Predication
- if-else code often is very short
- don't try to guess, just do both sides
- how to keep only the desired results?

### CMOV
- `CMOV R1, R2, R3`
- R1 = R3 ? R2: R1
- not good; too many registers (CMOV needed for each value)

### "Full" Predication
- every instruction is predicated
- separate predicate registers and separate instructions to set predicates
- do everything, and save the results based on the condition (every stage before MEM and WB is always executed)
- extra bits needed in every instruction
- must change ISA

### Exceptions
- when a predicate is false, and the nested instruction causes an exception, the exception must not be shown
- instruction must appear as a no-op

### if-conversion

#### benefits
- becomes branch-free code
- potential performance improvement??
- more scheduling flexibility

#### downsides
- guaranteed resource waste
- limited to small sequences

### Predict or Predicate?
usually we predict, but it depends

---

## Dependencies and ILP

### Instruction Level Parallelism (ILP)
- execute several instructions in parallel
- pipelining will only push through at most 1 instruction per cycle

#### Scope of ILP
- this is on a single thread level
- program level parallelism is the responsibility of the OS (executing different threads in parallel)
- running *sequential* code

#### "Correct" Excecution
processor state (registers, PC, memory) should be as if instructions were executed one at a time

### Superscalar and Vector CPUs
- "scalar" - executes one instruction at a time
- "vector" - one instruction at a time, but on vector data (e.g. a single register represents multiple data / dividing bits)
- "superscalar" - can execute more than one unrelated instruction at a time

### Modeling Stalls
- speedup = CPI<sub>no pipe</sub> / CPI<sub>pipe</sub>
- s = average number of stalls per instruction
- CPI<sub>pipe</sub> = CPI<sub>no stall</sub> (= 1) + s
- speedup = number of stages / (1 + s)

### False Dependencies
- a finite number of registers means overwriting is going to occur
- WAR and WAW are also called "name dependencies"; they come from reusing registers

#### Adding More Registers
- cannot be done because that would mean changing the ISA, and thus there is no more backwards compatibility (all previous code cannot be reused)
- does not address register overwriting from code reuse in loops and function calls

#### Register Renaming
- add more registers, but only in hardware (don't expose it to the ISA)
- temporarily map ISA registers to physical registers
- ARF (Architectural Register File)
- RAT (Register Alias Table) - holds a pointer to the appropriate register (either to the ARF, or the PRF if the register is being aliased)
- PRF (Physical Register File)

#### Register Reuse
- re-use can occur once the instruction that writes to the register retires from the pipeline
- however, the RAT cannot always be updated to point back to the ARF

### ILP
- number of instructions / longest path

### Dynamic (Out-of-Order) Scheduling
- window size - how many instructions ahead to look
- im tired of taking notes

---

## Dynamic Instruction Scheduling

### Scoreboarding
- do everything sequentially
- no register renaming
- detects WAR and WAW, but cannot eliminate (stalls)

#### Stages
- fetch - bring instructions from memory
- issue - decode instructions, and check for dependencies (stall if there are)
- read operands - wait until all of them are available
- execute - after the result is ready, the scoreboard is notified
- write result - delayed until earlier instructions that intend to read have completed their read operands stage

#### Data Structures
- instruction status - indicates which stage each instruction is in
- function unit status - indicates the state of each (9 fields)
- register status - which functional unit will write to the given register

### Tomasulo's Algorithm
- removes name dependencies through register renaming

#### Hardware
- instruction buffers
- register file, RAT (0 if copy from register file, 1 if waiting on result from a functional unit)
- 1 adder (3 slots), 1 Mul/Div (2 slots) - reservation stations

#### Issue
- get next instruction
- find a free reservation station (or stall if none are available)
- read operands from registers (or find the reservation station that will produce it)
- renames registers (reservation station IDs)

#### Execute
- monitor results as they are produced
- broadcast them to the reservation stations that are waiting on the given result; done by the CDB (common data bus)
- when more than 1 instruction is ready from the same unit, execute by oldest first

#### Write
- used CDB to make the result available (writeback to both register file and waiting reservation stations)
- update the register mapping (only if the RAT contains your mapping)
- free the reservation station

#### Load/Store
- reservation stations take care of register dependencies
- however, memory operations are not

#### Order of Execution
- in-order: issue and decode
- out-or-order: execute, writeback

### Problems with Tomasulo
- a lot, but basically you should separate register renaming from the reservation stations
- instructions are completed out of order, which makes exceptions and branch mispredication difficult to recover from (debugging is more difficult)

### Re-Order Buffer (ROB)
- separate architectural and physical registers
- allows for in-order completion

#### Hardware
- instruction buffers
- RAT (alias), which may point to ART or ROB
- ALUs and reservation stations (which will no longer point to other reservation stations, but the ROB instead)

#### Issue
- read instruction from buffer
- check for resources (appropriate RS and ROB entries, stall otherwise)

#### Execute
- same as before

#### Write Result
- broadcast result on CDB
- write result back to ROB entry only; the ARF will be updated in program order
- reservation station can be freed (or sooner, since the ROB keeps track)

#### Commit (new stage)
- occurs when an instruction is the oldest in the ROB
- make instruction execution "visible"

#### Register Renaming
- reservation stations can be freed earlier now (apply the same logic to the ROB)
- like before, update the alias only if it is pointing to the given entry

### Unified Reservation Stations
- instead of having separate reservation stations per functional unit, combine reservation stations to be able to hold values for either
- makes hardware a little more complex 

### Actual Superscalar Execution
- previous example can only reach a maximum of 1 IPC (only 1 CDB, superscalar fetch, decode, etc. needed)
- each stage must be able to handle more than 1 instruction at a time (Tomasulo can already do this for the execute stage, given that there are available functional units and instructions)

### Dual-Issue (Two-Way Superscalar)
- check resource availability for two instructions (RS and ROB entries)
- read for operands - most complicated (RAT/ARF/ROB + renaming)
- upate RS/ROB entries

#### Dual Rename
When there are no RAW dependencies, renaming is easy.
- use the current mappings in the RAT for each operand
- new destinations are just given the next two ROB entries (allocate `ROB_tail` and `ROB_tail + 1`)
```
    RAT         Original Instructions:    After Renaming:
    _______
R1 |_ARF1__|    R1 = R2 + R3              ROB21 = ROB17 + ARF3
R2 |_ROB17_|    R4 = R2 - R4              ROB22 = ROB17 - ROB6
R3 |_ARF3__|
R4 |_ROB6__|
```

However, when there are RAW dependencies, the operand that depends on a write value cannot simply use the given mapping in the RAT.
```
    RAT         Original Instructions:    After Renaming:
    _______
R1 |_ARF1__|    R1 = R2 + R3              ROB21 = ROB17 + ARF3
R2 |_ROB17_|    R4 = R1 - R4              ROB22 = ROB21 - ROB6
R3 |_ARF3__|                                       ^^
R4 |_ROB6__|                                    not ARF1
```

#### Multiple CDBs
- with multiple CDBs, each reservation station entry must be checked multiple times (once for each CDB) to verify whether it matches with the source of the CDB value
- this requires more area, logic, power ($$$)

#### Committting more than 1 IPC
Must check which instructions are ready to commit
| Head     | Next     | What Gets Committed |
| -------- | -------- | ------------------- |
| not done | -        | nothing             |
| done     | not done | head                |
| done     | done     | head and next       |

Must be able to write-back multiple results at once from ROB to ARF (or memory for stores)
- extra read ports for ROB, write ports for ARF

### Ordering in Each Stage
```
FETCH | DECODE | ISSUE     EXECUTE | WRITE      COMMIT

      in-order              out of order       in-order
```

---
## Interrupts, Exceptions, and Memory Dependencies

### Interrupts vs. Exceptions

#### Interrupts
- caused by external events
- asynchronous to program execution (unrelated to the program)
- simply suspend and resume user program

#### Exceptions
- caused by internal events
- synchronous to program execution
- system takes action to handle (handler)
- instruction may be retired and program continued, or the program is aborted

### Exception Generating Stages
| Stage   | Exceptions                                                                             |
| ------- | -------------------------------------------------------------------------------------- |
| fetch   | page fault on instruction fetch, misaligned memory access, memory-protection violation |
| decode  | undefined or illegal opcode                                                            |
| execute | arithmetic exception                                                                   |
| memory  | similar to fetch                                                                       |

### Exceptions Should be Precise
The state of the machine is preserved as if program executed up to the exception-causing instruction
- prior instructions are executed
- subsequent instruction did not modify process state
- exception causing instruction may or may not have been executed (depends on architecture and exception)

#### Treat Faults Like a Result
- at execution, make note of faults
- if the instruction gets to commit, *then* expose the fault

### Branch Misprediction
- must undo wrong-path register changes

#### Checkpointing
- at each branch, make a copy of the RAT

##### On Misprediction
1. mark wrong-path instructions in ROB
2. deallocate wrong-path RAT checkpoints
3. recover RAT from checkpoint
4. start fetching the correct instructions and resume

### Memory Dependencies
- only write stores during commit

#### Memory Disambiguation
- are there earlier unexcuted stores to the same address as the current load?

#### Store-to-load forwarding
- load: which earlier store to get value from?
- store: which later load(s) to transmit value to?

### Load Store Queue (LSQ)
- similar to the ROB
- loads cannot execute until all earlier store addresses are computed
- ST puts the value in the LSQ (for forwarding and commits)
- LD puts the value in the ROB

#### Issue
- allocate LSQ entry for each LD/ST (in addition to ROB entry, RS, etc.)

#### Execute
- generate address (update LSQ)
- produce result (update ROB)

#### Commit
- if ST, send value to memory
- move LSQ head

---
## Compiler ILP Techniques

Dependence chains limit ILP, and its hard to continue to modify hardware to improve it (ROB, RS limits).

### Tree Height Reduction
Shorten using associativity
```
   ADD R6, R2, R3          ADD R6, R2, R3
   ADD R7, R6, R4          ADD R7, R4, R5
   ADD R8, R7, R5          ADD R8, R7, R6

         I1
         |                     I1  I2
         I2                     \  /
         |                       I3
         I3

R8 = ((R3+R3)+R4)+R5    R8 = (R2+R3)+(R4+R5)
```

### Simple Loops
```c
for (i = 1000; i > 0; i--) {
    x[i] = x[i] + s;
}
```

#### Assume
- single-issue pipeline
- add to store: +2 cycles
- load to add: +1 cycle
- branch: +1 cycle

```
Loop:   LD  F0, 0(R1)                   Loop:   LD  F0, 0(R1)
        - stall                                 ADD R1, R1 #-8
        ADD F0, F0, F2                          ADD F0, F0, F2
        - stall            -------->            - stall
        - stall              hoist              - stall
        SD  F0, 0(R1)       the add        ---> SD  F0, 8(R1)
        ADD R1, R1, #-8                   |     - stall
        - stall                        offset   BNE R1, R2, Loop
        BNE R1, R2, Loop              modified
```

### Scheduling

#### Branches
Suppose a branch can go two ways, and one outcome requires a load. If waiting for the branch outcome takes many cycles, one option is to preemptively execute the load to before the branch. However, this would cause unnecessary execution when the branch goes in the other direction (aka **Dynamic Dead Code**).

#### Predication
Predication may help, but not when there is no "common case" (branch is 50-50).

Trying hardware stuff at the compiler level.

### Loop Unrolling
Transform an M-iteration loop into a M/N iteration loop.

```c
for (i = 1000; i > 0; i--) {
    x[i] = x[i] + s;
}
```

Unrolled:
```c
for (i = 1000; i > 0; i -= 4) {
    x[i] = x[i] + s;
    x[i+1] = x[i+1] + s;
    x[i+2] = x[i+2] + s;
    x[i+3] = x[i+3] + s;
}
```

#### Benefits
- fewer branches
- better scheduling of instructions (more instructions to choose from when reordering)
- get rid of small loops

#### Problems
- more code (more complicated to understand)
- What if N is not a multiple of M? (extra code)
- What if N is not known at compile time?
- while loops?

```c
for (i = 0; i < j; i++>) {
    a[i] *= 2;
}
```

### Function Inlining
- like "unrolling" a function (copy function code directly instead of calling the function)
- remove function call overhead
- larger block of instructions for scheduling

#### Similar Problems
- increase register pressure
- extra code
- recursive functions?

### Software Pipelining
Unrolling is not always enough.
```c
for (i = 0; i < 100; i++) {
    sum += a[i] * b[i];
}
```
#### Create Stages
- load, multiply, sum
```c
for (i = 0; i < 100; i++) {
    // Stage 1
    ai = a[i];
    bi = b[i];
    // Stage 2
    prod = ai * bi;
    // Stage 3
    sum += prod;
}
```

#### Pipelining

The code in the slides makes no sense?????

This is basically just getting rid of RAW dependencies.

```c
// setup variables
prod = a[0] * b[0];
a = a[1], b = b[1];

// the last product to be added from the loop is a[97] * b[97]
for (i = 2; i < 100; i++) {
    sum += prod;         // sum += a[i-2] * b[i-2]
    prod = a * b;        // prod = a[i-1] * b[i-1]
    a = a[i]; b = b[i];  // current iteration
}

// finish up the last two iterations
sum += prod;  // sum += a[98] * b[98]
sum += a * b; // sum += a[99] * b[99]
```

#### Problems
- more code
- register pressure
- must know exit condition
- works only for loops

---
## Virtual Memory and Memory Protection

you learned this already
- programmer is unconcerned about memory (virtual > physical)
- memory is divided into pages
- page tables to map (w/ permissions)

### Page Tables
- single page table takes up a lot of space
- use multi-level page tables instead
- more space is needed for multiple levels, but space is saved because not all entries need to be allocated (thus removing entire tables)

#### Page Size
- small pages: bigger page table, but less fragmentation
- big pages: efficient transfer to/from disk

### CPU Memory Access
1. compute virtual address (adding offset)
2. compute virtual page number
3. compute physical address of VPN's page table entry
4. load mapping (for each table level)
5. compute physical address
6. do the actual load from memory

#### Performance Impact
- each load/store requires at least 2 accesses
- each fetch requires translation
- however, once a virtual page is mapped, it will most likely remain for awhile

### TLB (Translation Look-Aside Buffer)
- caching translations to avoid the performance impact

What happens when there is a TLB miss?
- hardware (fast) - register to keep table location, and hardware can read the page table (stall until translation is found)
- software (slow) - raise an exception, use a miss handler in the OS, and then retry the instruction

#### TLB Design

##### Fully Associative (old)
- any mapping can be kept in any entry
- check all entries on each access
- few entries to maintain low latency
- how big should the working set be to avoid TLB misses?
- if there are many misses: increase TLB size (latency problems), increase page size

##### Set-Associative (current)
- programs are getting bigger, so the TLB must remember more mappings
- mapping goes to a specific part of the TLB using lowest VPN bits
- some use multi-level TLBs

#### The TLB is not a Cache
- the TLB saves only the final translation
- a cache can save the middle layers (the mapping between page tables)

#### Process Changes
- the TLB is invalid after a context switch
- instead of flushing the entire TLB, add a PID to the translation to check validity
- only flush when recycling PIDs

### Memory Protection
- permission bits for each page, kept in page table and TLB entry, checked during translation

#### OS Considerations
- system mode: access to physical memory, and allows execution of special instructions
- user mode: can't do those things

---
## Caches

### Locality
- **temporal** - if data item needed now, likely needed again in near future
- **spatial** - if data item needed now, nearby data likely needed in near future
- caches - keep recently used data in fast memory close to processor (and bring nearby data)

|             Storage Hierarchy             |
| :---------------------------------------: |
|                   Disk                    |
|                Main Memory                |
|                 L3 Cache                  |
|                 L2 Cache                  |
| ITLB, Instruction Cache, Data Cache, DTLB |
|               Register File               |
|              Bypass Network               |
- from high capacity, low speed, to low capacity, high speed

### Cache Basics
- you know this already
- if something is not in the cache, bring in the entire block of data

#### Important Decisions
- placement: where does it go?
- identification: how to find a block in the cache?
- replacement: what gets kicked out to make room?
- write policy: how to handle stores?

#### Structure
- block-sized lines
- blocks are typically 16-128 bytes in size (most commonly 64)
- use address offset to index within block

### Cache Placement
How to assign memory blocks to cache lines
- direct mapped (each block can go to one line only)
- fully associative (any block can go to any line)
- set-associative (block can go to one of N lines)

#### Direct Mapped
Each block can go to one line only.
```
           ______________ _______ ________________
Address   |_____Tag______|_Index_|_____Offset_____|
                 |           |            |
              hit or     select bits     block
                miss      for cache      size
```
Tags are really expensive in hardware, there are a lot of bits to compare.

#### Fully Associative
```
           ______________________ ________________
Address   |__________Tag_________|_____Offset_____|
```
The block can go anywhere in the cache (there is no more index), so each line in the cache must be checked (larger tag as well).

#### N-Way Set Associative
- N represents the size of each set
```
           ______________ _______ ________________
Address   |_____Tag______|_Index_|_____Offset_____|
                             |
         _____            to index
        |  0  | set 0     into set
        |__1__|          (2 bits for
Cache   |  2  | set 1      4 sets)
Line #  |__3__|
        |  4  | set 2
        |__5__|
        |  6  | set 3
        |__7__|
```
- 1-way set associative = direct mapped
- N-way set associative = fully associative

### Cache Identification
- cache lookup - find whether data is in the cache, and where
- each cache line has a **valid** bit, and a **tag** to identify which block is in the line

### Examples
1. **Direct mapped, 32-byte line, 256 bytes total (cache size)**
   - block offset is 5 bits (32-byte line)
   - 8 lines (256/32)
   - index is 3 bits (for 8 lines)
   - rest of the address is tag bits (8..63)

    When accessing the cache:
    - use index bits to find the line
    - use tag bits to check if they match
    - if no match, fetch block from memory
    - use offset bits to get correct block in line

2. **Fully-associative, 16-byte line, 1024 bytes total**
   - block offset is 4 bits (from 16-byte line)
   - 64 lines (1024/16)
   - no cache index, so bits 4..63 are tag

   When accessing the cache:
   - use tag bits to check all lines for a match

3. **8-way set associative, 64-byte line, 2KB total**
   - 6 bit block offset (64 byte line)
   - 32 lines (2048/64)
   - 4 sets (32 lines / 8 lines per set)
   - 2 bit index (for 4 sets)
   
   When accessing the cache:
   - use index bits to identify which set
   - use tag bits to find if any lines within the set match

### Cache Replacement
Which block gets kicked out when a free line is needed?
- random
- FIFO (line that has been in the cache the longest)
- **LRU (least recently used)**
- LRU Approximations
- NMRU (not most recently used)
- LFU (least frequently used)

#### Implementing LRU
- have LRU counter for each line in a set

When a line is accessed:
- get old value of its counter
- set counter to max value
- for every other line in the set greater than the old value, decrement by 1

When a line is needed, select the one with the counter at 0.

### Write Policy
Do we allocate cache lines on a write?
- write-allocate - write miss brings block into cache
- no-write-allocate - write miss leaves cache alone

Do we update memory on writes?
- write-through - memory immediately updated on each write
- write-back - memory updated when line is replaced

Write-allocate + write-back vs. no-write allocate + write-through

#### Write-Back Caches
- dirty bit for each line (dirty if line has been written to and memory has not been updated)
- replacing a dirty line - must write-back to memory

### TLB Interaction
- virtual vs. physical to access the cache?

#### PAPT (Physically-Addressed Physically-Tagged)
- translate the virtual address, and use the physical address to index into the cache
- slow, bc translation may be slow

#### Virtually Addressed
- fast; no need to check TLB
- must flush cache on process change (very costly)
- hard to enforce permissions (TLB isn't being checked)
- aliasing problem - several virtual pages can map to the same physical page

#### Virtually Indexed Physically Tagged
Index into cache with the virtual address, while translating simultaneously to get the physical address. Then, compare the physical address to the tag from the cache.
- fast; TLB parallelized with caching
- no need to flush the cache on a process swap
- aliasing is still an issue

---
## Cache Performance
- **average memory access time = hit time + miss rate * miss penalty**
- cpu time = cycle time * (cycles<sub>exec</sub> + cycles<sub>memory_stall</sub>)
- cycles<sub>memory_stall</sub> = cache misses * (miss_latency<sub>total</sub> - miss_latency<sub>overlapped</sub>)

### Improving Cache Performance
- reduce hit time, miss penalty, or miss rate
- increase the overlapped miss latency (from cycles<sub>memory_stall</sub>)

#### Example
Memory latency = 100 cycles, 16KB cache, 3 cycle latency, 85% hit rate
```
AMAT = 3 cycles + 0.15 miss rate * 100 cycles = 18 cycles
```

What's the best way to reduce latency?
- smaller 8KB cache: 1 cycle latency, 75% hit rate
- larger 32KB cache: 4 cycle latency, 90% hit rate

```
smaller cache: 1 cycle + 0.25 * 100 cycles = 26 cycles
larger cache: 4 cycles + 0.10 * 100 cycles = 14 cycles

The larger cache is the best way to reduce latency (the smaller cache is actually less efficient than the original).
```

### Reducing Hit Time
- small and simple caches are faster
- index using virtual address
- pipelined caches (improves bandwidth, not latency; essential for L1 caches at high frequency)
- trace caches: for instruction caches
- way prediction: to speed up set-associative caches; guess which item within set

### Reducing Miss Penalty

#### Multi-Level Caches
- v. fast small L1, fast not so small L2, etc. (stacking penalties)
- global v. local miss rate (comparing with all memory references v. memory references that make it to the given cache)
- exclusion property: if block is in L1, it is never in L2 (saves L2 space)
- inclusion property: if block is in L1, it must also be in L2 (simplifies logic)
- or you can enforce neither property

##### Example
Memory latency 100 cycles
Using 1 cache:
- 16KB L1 cache, 3 cycle latency, 85% hit rate.
```
AMAT = 3 + .15 * 100 = 18 cycles
```
Two Levels of caching:
- smaller 8KB cache: 1 cycle latency, 75% hit rate
- larger 128KB cache: 6 cycle latency, 60% hit rate

```
AMAT = 1 + 0.25 * AMAT_L2
AMAT_L2 = 6 + 0.40 * 100 = 46 cycles
AMAT = 1 + 0.25 * 46 = 12.5 cycles
```

#### Sectored Caches
- divide a block into sectors (separate valid and dirty bits for each)
- bus is too narrow, so give data to loads earlier
- **early restart**: when needed word arrives, let processor use it, then continue block transfer
- **critical word first**: transfer word first, then the rest of the block

#### Increase Load Miss Priority
- loads usually have dependent instructions
- if a load mises and a store needs to go to memory, let the load go first
- write buffer needed to remember stores

Merging write buffer: 
- if there are multiple write misses to the same block, combine them in the write buffer
- use block-write instead of many small writes

### Reducing Cache Misses
| Miss Type  | Cause                                                              |
| ---------- | ------------------------------------------------------------------ |
| compulsory | first time each block is accessed                                  |
| capacity   | limited cache capacity (would not occur w/ infinite cache size)    |
| conflict   | limited associativity (would not occur w/ fully associative cache) |

#### Victim Caches
- recently kicked-out blocks kept in small cache
- works for conflict misses

##### Example
Direct-mapped L1 cache, 16-line fully associative victim cache
- prevents thrashing when several blocks want to go to the same entry

#### Other Modifications
- larger block size: helps if there is more spatial locality
- larger caches: fewer capacity misses, longer hit latency
- higher associativity: fewer conflict misses, longer hit latency

#### Pseudo Associative Caches
- similar to way prediction
- start with direct mapped, if miss, try another entry
- prof. doesn't know why this works

#### Compiler Optimizations
- loop interchange - for loops over multi-dimensional arrays (change order of iteration to get better spatial locality)
- blocking

#### Hiding Miss Latency
- overlap miss latency with useful work
- non-blocking caches - pending request types (hit under miss, miss under miss, hit under multiple misses)
- prefetching - predict what will be needed and bring it ahead of time

#### Miss Status Handling Register (MSHRs)
- keep track of outstanding cache misses, pending load/store accesses that refer to the missing cache line
- fields: there are a lot; look at the slides
- cache will use it to check if a miss to the same block is already pending
- similar to a load/store queue; allows overlapping of memory accesses
- between memory and cache
- check the MSHR during cache access and after seeing a cache miss

---
## Prefetching
If memory takes a long time, start accessing earlier.
- what to prefetch, when, and where to put it?

### Software Prefetching
- register vs. cache
- each can be faulting or non-faulting (are there exceptions w/ a bad address?)
- faulting register prefetch is binding (address must be OK)
- non-faulting cache prefetch is non-binding (bad address becomes noop, more overhead)

#### Cache
- load instruction remains, ISA change needed (prefetch instruction), cache gets more complicated (prefetches vs. loads)
- predict future misses and get data into cache
- if access occurs, then there is a hit; if not, **cache pollution**
- to avoid pollution, use prefetch buffers (most often used)
- use L2 cache as a prefetch buffer

#### Intrinsics
```c
for (i = 0; i < N; i++) {
    ip += a[i] * b[i];
}
```
- insert "assembly-like instructions" into high level source code

##### Basic
```c
for (i = 0; i < N; i++) {
    fetch(&a[i+1]);
    fetch(&b[i+1]);

    ip += a[i] * b[i];
}
```
- multiple requests made for the same cache block, so kind of useless
- missing a[0] and b[0]

##### With Loop Unrolling + Prologue/Epilogue
```c
fetch(&a[0]);
fetch(&b[0]);

for (i = 0; i < N-4; i += 4) {
    fetch(&a[i+4]);
    fetch(&b[i+4]);

    ip += a[i] * b[i];
    ip += a[i+1] * b[i+1];
    ip += a[i+2] * b[i+2];
    ip += a[i+3] * b[i+3];
}

for (; i < N; i++) {
    ip += a[i]*b[i];
}
```
- one request per cache block

#### Prefetch Distance
- one iteration early may not be enough; depends on memory latency and amount of computation between accesses
- prefetch distance = ceiling(average memory latency / shortest estimated cycle time)
- problem: memory latency varies at run-time (contention for resources)

#### Linked List
```c
for (i = 1; i < N; i++) {
    listNode *p = listHead[i];
    while (p) {
        prefetch(p->next->next->next); // assume prefetch takes 3 iterations
        work(p->data);
        p = p->next;
    }
}
```

### Prefetch Metrics
- useful prefetches = number of blocks that will be used by demand loads

| Metric     | Definition                                 |
| ---------- | ------------------------------------------ |
| accuracy   | useful prefetches / total prefetches       |
| coverage   | useful / cache misses                      |
| timeliness | how close to the load the prefetch arrives |

### Software Prefetching Limitations
- compiler or programmer needs to insert (usually limited to loops)
- prefetch overhead
- code expansion
- static decision; cache sizes vary between machines

### Hardware Prefetching
- monitor miss traffic to DRAM
- depending on prefetch algorithm/miss patterns, prefetcher injects additional memory requests
- cannot be overly aggressive; fighting for memory bandwidth, may pollute the cache

Different Algorithms:
- demand memory addresses: stream/stride
- PC addresses?
- memory values: content based
- old address histories: Markov prefetching

#### Stream/Stride
- stream: if missed 1, 2, 3, 4, prefetch 5, 6, 7
- stride: if missed 1, 4, 7, 10, prefetch 13, 16, 19
- fairly easy to implement in hardware

#### Instruction Prefetching
- instructions are sequential, and fairly easy to predict
- next line prefetcher (one block ahead)

#### Markov Prefetching
- form address correlations
- use global memory addresses as states in the Markov graph
- data structures: pointer, linked list
- history based: space required; expensive

#### Global History Buffer (GHB)
- unified frame for different types of prefetchers
- holds miss address history in FIFO order
- linked lists within GHB connect related addresses

#### Content-Directed Prefetching
- pointer prefetching scheme
- look for data that might represent memory addresses

#### Pre-Computation Based Prefetching
- speculative execution (high accuracy and good coverage; no architectural changes)
```c
for (i = 1; i < N; i++) {
    listNode *p = listHead[i];
    while(p) {
        work(p->data); // pre-emptively execute everything except this line
        p = p->next;
    }
}
```

#### Runahead Execution
- instructions are speculatively pre-executed to generate prefetches
- happens when the oldest instruction is an L2 miss; ends when original L2 miss returns
- parallel execution while waiting for L2 (saving future cycles)

### Prefetching Overhead
- software: extra instructions, cache pollution, bandwidth consumption
- hardware: transistors, cache pollution, bandwidth

---
## Memory

### Memory Bank Organization
```
          MSB    _____________________
address ------> |                     |
   |      row   |     2D Storage      |
   |    decoder |        Array        |
   |            |_____________________|
   -------------->   column decoder
                   (row buffer + mux)
                           |
                       data out
```

#### Read Sequence
1. decode row address and drive word-lines (the row)
2. selected bits drive bit-lines (the column) - entire row read
3. **amplify row data**
4. decode column address and select subset of row (sent to output)
5. **precharge bit-lines for next access**

### SRAM (Static Random Access Memory)
- register file, caches

```
     |    row select    | 
_____|__________________|___     6 transistors total
     |  |   __>.__   |  |
     |_[o]_|      |_[o]_|        [o] = driving
     |     |__.<__|     |             transistor
     |                  |
  bitline            bitline
```
- inner circuit (two inverters; 1 | 0 or 0 | 1) is always charged (consuming power)

#### Read Sequence
1. address decode
2. drive row select
3. selected bit-cells drive bitlines (bitlines set to inner circuit values)
4. diff. sensing and col. select (compare bitlines to determine the value)
5. precharge all bitlines (reset bitline value to 1/2) for the next use

### DRAM (Dynamic Random Access Memory)
bits stores as charges on node capacitance (non-restorative)
- bit cell loses charge when read, and loses charge over time
- **refresh** - DRAM controller must periodically read all rows within the allowed refresh time so that the charge can be restored in cells
- destructive read - afterwards, cell contains something close to 1/2
```
   |
___|__________ row
   |   |      enable      2 transistors total
   |__[o]__
   |       |    (over time)
   |       =    1 ~~~~~~~ 0
   |       |
   |       v
bitline
```

#### Read Sequence
1. "
2. "
3. "
4. a "flip flopping" sense amplifier detects if the bitline is slightly greater than 1/2 (also "recharges" the capacitor); data bit is mux'ed out
5. precharge all bitlines

### SRAM vs. DRAM
- SRAM: fast access, no refreshes, simpler manufacturing, lower density (6 transistors per cell), higher cost
- DRAM (stand-alone memory chips): higher capacity, higher density, lower cost

### Memory Subsystem Organization
| Hierarchy  |
| :--------: |
|  channel   |
|    DIMM    |
|    rank    |
|    chip    |
|    bank    |
| row/column |

- processor connects to memory via memory channels (generally just 1 or 2)
- each channel has ideally 2 DIMMs
- 2 sides (ranks)
- rank will hold 8 chips (64 bits total) - respond to a single command
- each chip is activated in parallel ("invisible" construct)
- all chips have multiple banks
- banks (see [diagram](#memory-bank-organization)) have a row-buffer 

### Transferring a Cache Block
- 64B cache block takes 8 I/O cycles to transfer (data bus is 64 bits)
- 8 columns are read sequentially

### Page Mode DRAM
- DRAM row is also called a DRAM page (similar in size to OS page)
- "sense amplifiers" are also called "row buffers"; stores a page
- each address is a <row, column> pair

#### Access to a "closed row"
- activate - opens row (placed into row buffer)
- read/write - read/write column in the row buffer
- precharge - closes the row and prepares the bank for the next access

#### Access to an "open row"
- no need for activate command

### Latency Components (DRAM Operation)
- CPU -> controller transfer time
- controller latency: queue + scheduling delay (where to send), convert to basic commands
- controller - DRAM transfer time
- DRAM bank latency
  - CAS - open row
  - RAS + CAS - precharged (bring row into row buffer)
  - PRE + RAS + CAS - row buffer conflict (clear buffer)
- DRAM -> CPU transfer time

#### Open vs. Close Page Policies
- open: keep row opened until conflict (hit = CAS, miss = PRE + RAS + CAS)
- close: close row buffer after access (miss = RAS + CAS)
- policy depends on row buffer hit rate (laptop will typically be open policy)

### DRAM Read Timing
- state machine (sending columns one by one); send row, send column, send data out
- burst - send many columns at once
- DDR (double-data rate) - transfer data on both rising and falling edge of the clock

#### Burst Access
- one command access, multiple bytes are read/written
- hardware provides multiple burst length options that are set by software

### DRAM Refresh
- capcitor charge leaks over time; periodically restoring charge
- activate + precharge each row; typically every 64 ms

#### Impact
- bank unavailable during refresh
- long pause times - if refreshing all rows in burst, every 64ms the DRAM will be unavailable for the refresh time

#### Refresh Strategy
- burst - all rows refreshed immediately after one another
- distributed - spread out refresh evenly across interval; eliminates long pause times

### Memory Controller
- read and write queue
- scheduler sends the commands to DRAM (very complicated)
- buffer -> response queue

#### Scheduling Policies
- FR-FCFS (first ready, first come first served) - row-hit first; maximize row buffer hit rate (maximize **throughput**)

#### Design Difficulties
- DRAM timing constraints for correctness (waiting to issue a read after write, time between two consecutive activate commands to same bank)
- track resources to prevent conflicts
- DRAM refresh
- optimizing for performance (reordering is complicated)

### Multiple Banks and Channels
- enables concurrent DRAM accesses
- increased bandwidth
- disadvantages: higher cost (more board wires, more pins)

### Address Mapping (Single Channel)
single channel with 8-byte memory bus (2GB, 8 banks, 16K rows + 2K columns per bank)

#### Row Interleaving
- consecutive rows of memory in consecutive banks
- optimizing access of sequential data

```
|       Row       | Bank |       Column       | Byte in bus |
```

#### Cache Block Interleaving
- consecutive cache block addresses in consecutive banks
- 64 byte cache blocks
- accesses to consecutive cache blocks can be serviced in parallel

```
|       Row       | High Col | Bank | Low Col | Byte in bus |
```

#### Bank Mapping Randomization
- lower chance of bank conflicts

### Storage Types
#### SSD
- expensive, wears over time in ways that are hard to detect
#### Magnetic Disks
- cheaper, reliable
- you kind of learned this


---
## Reliability
- fault - cause of error 
- error - defect that results in failure
- failure - actual deviation from specified behavior

Example:
- fault - programming mistake (add function that works until given 5 + 3, when it returns 7); latent error
- error - call the function on 5 + 3; effective error
- failure - error results in deviation from behaviour (scheduling a meeting for the 7th instead of the 8th)

### Reliability and Availability
- system can be in two states: service accomplishment, service interruption
- reliability - measure of continuous service accomplishment
  - mean time to failure (MTTF)
- availability - service accomplishment as a fraction of overall time
  - uses mean time to repair (MTTR)
  - availability = MTTF / (MTTF + MTTR)

### Faults
#### Classified by Cause
- hardware faults (fail to perform as designed)
- design faults - faults in software (and some in HW)
- operation faults - operator and user mistakes
- environmental faults - fire, power failure, etc.

#### By Duration
- transient - last for a limited time; not recurring
- intermittent - recurring, but last for a limited time (overclocked system works fine, crashes, and works again upon restart but will occur again)
- permanent - do not get corrected when time passes

### Improving Reliability
- fault avoidance (by construction), tolerance (prevent from becoming failure; by redundancy)
- error removal (by verification), forcasting (by estimating presence, creation, and consequences)

### Memory Error Sources
- soft error (alpha particle)
- retention errors (capacitor charge loss)
- hard errors (manufacturing)

### Coding for Reliability
- error detection vs correction code (EDC vs ECC)
- EDC - parity code, bit interleaved parity, checksum errors, berger code (unidirectional errors)
- ECC - hamming code, SECDED

### EDC
#### Parity Code
- taack if the number of 1s in code is odd or even
- storage overhead: 1 bit per code work
- detect odd number of bit flips
- particle strike can flip adjacent bits; errors cancel --> bit interleaved parity

#### Checksum Code
- split data into equal sized words; checksum computed as XOR
- vulnerability - even number of bit flips in the same position --> use other hashing function instead
- storage overhead: word size

#### Berger Code for Unidirectional Errors
- unidirectional - one direction only; e.g. DRAM only goes from 1 to 0, not 0 to 1
- berger code - detects any number of unidirectional errors; count number of 1s, invert and store
- number of 1s can only go down; and in stored code can only go up
- storage: log(n+1) in n bits

### ECC
- detect *and* correct errors

#### Replication
- Modulo Redundancy (TMR) - copy everything twice (plus the original)
- if bits differ, take the majority

#### Hamming Code (7,4 Code)
- define valid codewords (minimum distance between each is 3) out of all possibilities (16 out of 128) 
- datarate = data bits / total bits = 4/7
- im confused

#### SECDED (single error correction, double error detection)
- SEC can either either correct one error, **or** detect two errors (not both); correcting 2 errors increases the error number to 3
- put parity code on top of SEC to detect 2 errors (extended hamming)

### Reducing Cost of Error Correction
- clean lines vs. dirty lines (clean only need EDC, dirty needs ECC)

### Disk Fault Tolerance with RAID
- redundant array of independent disks (multiple smaller disks to play role of one big disk)
- can improve performance (access in parallel)
- can improve reliability (data kept w/ redundancy)

#### RAID 0
- striping?
- disks share the load (all disks work in parallel)
- no redundancy - reliability lower than single disk (if either disk fails, than the entire thing fails)

Reliability:
- p = probability that a drive will die in one hour
  - for a single drive: MTTF (mean time to failure) = 1/P hours
For RAID 0 with two drives:
- both drives survive an hour: (1-P)<sup>2</sup>
- probability of failure: 1-(1-P)<sup>2</sup>
- MTTF = 1 / (1-(1-P)<sup>2</sup>)

#### RAID 1
- disk mirroring - two copies of the same thing (write must update both, read can read either)
- improved performance (for reads) and reliability (2 copies)

Reliability:
- MTTF = 1/P<sup>2</sup> (only if replacing failed drives in the same hour)

#### RAID 0 and 1
- if there are more than 2 disks
- "striped mirrors" (1 + 0) - pair disks for mirroring, stripiing across the 4 pairs
- "mirrored stripes" (0 + 1) - striping with 4 disks, then mirror using the other 4

#### RAID 4
- block interleaved parity (save one block as a parity block across the rest of the data blocks)
- writes must also write to the parity block
- can recover from any error, hold more data
- write performance is much worse

#### RAID 5
- distributed block-interleaved parity
- parity block is distributed across all disks
- write still has to update the parity block; but now the load is shared by all disks

Reliability:
- no drives fail: (1-P)<sup>4</sup>
- exactly 1 fails: 4P * (1-P)<sup>3</sup>
- 2 or more drives fail: 1 - [(1-P)<sup>4</sup> + 4P*(1-P)<sup>3</sup>]

---
## Multiprocessing and Multithreading

### Software Problem
- most code is sequential
- parallel code is hard to write, harder to make efficient (and correct), and even harder to debug

### MIMD Multiprocessors
- centralized shared memory
- distributed memory

#### Centralized-Memory Machines
- data sharing through memory read/writes
- "uniform memory access" (UMA) - all memory locations have similar latencies
- problem: memory contention between processors; memory bandwidth is a bottleneck

Pros:
- communication happens automatically
- more natural way of programming
- no need to manually distribute data

Cons:
- need more hardware support
- easy to write correct, but inefficient programs (remote vs local accesses appear the same)

#### Message-Passing Machines (distributed)
- each processor has its own local memory
- to communicate with other processors, explicitly send/receive messages via an interconnect
- standard libraries for message passing
- two types of send primitives: synchronous (P1 stops until P2 confirms receipt of message), asynchronous (P1 sends its message and continues)

Pros:
- simpler and cheaper hardware
- explicit communication makes programmers aware of costly operations

Cons:
- explicit communication is difficult to program
- requires manual optimization (declare variables to be local and accessible, available to other processors for send/receive)

#### Distributed Shared Memory
- provide shared memory to distributed system
- use software memory manager to provide the illusion of shared memory (or hardware - e.g. NUMA)

### Thread Level Parallelism
- good for shared memory
- loop parallelism - threads get assigned different iterations of a loop
- function parallelism - threads get assigned a subroutine to work on

### A Message Passing Program
```c
// manually split the array
#define ASIZE 1024
#define NUMPROC 4

double array[ASIZE/NUMPROC];
double sum = 0;

// each processor computes its own results
for (int i = 0; i < ASIZE/NUMPROC; i++)
    sum += array[i];

if (PID != 0) {
    // processors send their results to the master
    send(0, sum);
}

// main processor adds up partial sums and prints the result
else {
    for (int p = 1; p < NUMPROC; p++) {
        int pSum;
        recv(p, pSum);
        sum += pSum;
    }
    printf("sum: %;f\n", sum);
}
```

#### In a shared memory system:
```c
// the array is shared
#define ASIZE 1024
#define NUMPROC 4

shared double array[ASIZE];
shared double sum = 0;

double mySum = 0;

// each processor sums its own part of the array
for (int i = myPID*ASIZE/NUMPROC; i < (myPID+1)*ASIZE/NUMPROC; i++)
    mySum += array[i];

// each processor adds its partial sum to the result
allSum += mySum;

// main processor prints the result
if (myPID) {
    printf("sum: %;f\n", sum);
}
```

### Communication Performance
- metrics: bandwidth, latency (send/receiver overhead + transfer time), latency hiding
- communication to computation ratio (work done vs. bytes sent over network)

### Parallel Performance
- serial sections (difficult to parallelize entire app)
- large remote access latency
- something about remote requests?

### Shared Memory Multiprocessing
- needs architectural support
- multi-core almost always uses shared memory

### Shared Memory vs Message Passing

| Area               | Shared Memory         | Message Passing     |
| ------------------ | --------------------- | ------------------- |
| communication      | implicit (load/store) | explicit (messages) |
| synchronization    | explicity             | implicit (messages) |
| hardware support   | required              | optional            |
| development effort | lower                 | higher              |
| tuning effort      | higher                | lower               |

---
## Cache Coherence

### Using multiple processors
- avoids cache thrashing
- no contention for processor resources
- however, caches store their own copies of the data in memory, so if one processor modifies a value that a different processor has in its own cache, the cache will now have an incorrect value

### Cache Coherence Definition
1. read what is written 
   - preserve program order
   - if there is no sharing, each processor acts like a uniprocessor
2. writes happen eventually
   - any write to an address must eventually be seen by other processors
   - values are propagated
3. causality of writes
   - writes to the same location are serialized (same order should be seen by all other processors)
   - if different processors could see write in a different order, they might think the final value of X is the wrong value

### Maintaining Cache Coherence
- shared caches - trivially enforces coherence, but not scalable
- snooping - each CPU has its own cache, and a broadcast network is needed to enforce coherence
- directory - like snooping, but with a point-to-point network

### MSI Snoopy Protocol
- each cache block has a coherence state (modified, shared, invalid - MSI)
- state determines what permissions we have
- caches send messages to gain permission

#### States
- invalid - block is not cached; to read/write, make a request on bus
- shared - block is cached and clean; other caches may have the block; no need to update memory when replacing B; to write, send an upgrade request (to M state)
- modified - block is cached and dirty; no other cache has the block; must update when replacing; can write without going to the bus

| State    | Permission | My Copy        | Others                 | Memory                 |
| -------- | ---------- | -------------- | ---------------------- | ---------------------- |
| invalid  | none       | invalid        | unknown                | unknown                |
| shared   | read       | correct, clean | maybe (correct, clean) | up-to-date             |
| modified | read/write | dirty          | none                   | stale (not up-to-date) |

#### Bus Actions
- caches must send coherence requests to change states
- `getS` - usually read miss; get with intent to share
- `getM` or `getX` - writes; get with intent to modify

#### Cache to Cache Transfers
- processor (P2) wants to read a block that is in M state in another processor's cache (P1)
- solution 1 - abort/retry - P1 cancels P2 request, issues write-back (too slow)
- solution 2 - intervention - P1 indicates it will supply the data; P2 sends the data on bus, memory is updated, P2 snoops the transfer

#### Problem
- multiple writes to the same cache block are inefficient, since an intervention by another cache will kick out the block in the given cache

### MOSI Protocol
- adding the Owner state (O)
- data is valid and shared (like in S)
- data is dirty (memory is stale, like M)
- intervene to provide data on getS, but go to O state instead of I
- fixes the MSI problem - better for write-heavy applications

### MESI Protocol
An optimization, by adding the E (exclusive) state.
- data is valid and clean (like S state)
- data is the only cached copy (like M state)
```
Potential Situation:
P1 does a getS, and no one else does a getS.
P1 wants to modify the data, and does a getM.
```
- performing getM is unnecessary, since no other cache has the data
- the exclusive state means that P1 has the only clean copy besides memory, and allows a "silent updgrade" to M 

#### "Share" Bus Signal
How do you choose between going to the S or E state?
- a "hardwired or"; the bus is always 0, unless someone pulls it to 1
- when a cache snoops a request and sees that it has the same data, it will pull "share" to 1
- the requesting cache will go to S is "share" is 1, and E if it is 0

### MOESI Protcol

| Letter | State     | Description                                                                                                 |
| ------ | --------- | ----------------------------------------------------------------------------------------------------------- |
| M      | Modified  | I have the only copy, and it's dirty.                                                                       |
| O      | Owned     | I have the most up-to-date copy (others may too), but I am responsible for supply data and updating memory. |
| E      | Exclusive | I have the only copy, and it's clean.                                                                       |
| S      | Shared    | There are multiple copies, and they're all clean.                                                           |
| I      | Invalid   | Data is invalid.                                                                                            |

### Write Policies
- write-update vs. write-invalidate

#### Write Update
- write to shared data is broadcast to update all copies
- costly to broadcast each write

#### Write Invalidate
- write to shared data forces invalidation of all other cached copies
- subsequent reads miss and fetch new value
- writes ordered by invalidations on the bus

#### Which One?
Depends on the application, but write-invalidate is simpler and more common.

Burst of writes by a processor to one address.
- update - each sends an update (makes no sense)
- **invalidate** - possibly only first invalidation is sent

Write to different words of a block
- update - upon on each word
- **invalidate** - possibly only first invalidation is sent

Producer-consumer
- **update** - producer sends an update, consumer reads new value from its cache
- invalidate - producer invalidates consumer copy, consumer has a read miss and has to request the block

### Shared Memory Performance
- another "C" for cache misses
- Compulsory, Capacity, Conflict + Coherence
- coherence miss - it was in the cache, but invalidated

### False Sharing
- true sharing - different processors access the same data
- false sharing - different processors access different data, but they happen to be in the same block
- worsens if block size is increased

---
## Directory-Based Cache Coherence
- point-to-point networks do not have observable broadcasts (`getS` and `getM` won't work)
- there are no ordered messages - which request comes first?
- for every memory block, the directory has an entry
- entries held at home node (memory)
- indicates state of the block, and who has cached copies of it
- for P processors and S states, metadata is P + log(S) per block

### MSI Read Miss
Processor has a read miss, request sent to directory.  
Depending on the state of the block, the directory controller will:
- I - read from memory and send back
- S - read and send back
- M - request forwarded to the owner node

Look at the state diagrams from the slides.

### MSI Write Miss
Processor has a write miss, request sent to directory.  
Directory controller (according to block state):
- I - read from memory and send back
- M - forward to owner node, owner sends data to processor + memory
- S - read from memory and send back, along with # of sharers' invalidate all sharers (which must acknowledge the invalidation - needed because the network is not ordered)

### MSI Replace Block
Processor needs to kick out a block, directory request.
- directory must remove sharer bit
- PutS (clean data) / PutM+Data (dirty data)

Directory Controller:
- acknowledges Put request
- removes sharer bit
- may also change state to I

### Messages in Directory Coherence

#### Retrieving
`getS`/`getM`
- getting data from memory

`fwd-getS`/`fwd-getM`
- directory forwards request to sharer
- like "observing" request
- downgrade the state (M->S)

#### Invalidating
`inv`
- directory sends invalidate to sharer
- like "observing" `getM` request
- downgrade S->I

`inv-ack`
- network is not ordered
- when did the sharer receive the request?


#### Removing
- `putS` - sharers need to be explicitly removed from directory
- `putM` - like writeback from snoopy
- `put-ack` - needs to wait until directory receives request

### Directory Location

#### Centralized
- next to memory controller
- poor scalability
- for fewer cores

#### Distributed
- nodes responsible for different slices
- "Home Node"
- based on physical address

### Reducing Directory Overhead
```
Remember that metadata is P + log(S) per block.
With a 64-byte block:
- 64 nodes -> 64 + log(S) =     12.5% +
- 256 nodes -> 256 + log(S) =     50% +

With more processors, metadata becomes too costly.
```

#### Limited Pointer Scheme
- generally, the number of sharers is much less than P
- keep IDs instead of bits for each processor
- store inode IDs
- i * log(P) < P
- broadcast when there is overflow

#### Grouping
- we don't really need one bit per P
- group processors into nodes
- broadcast to all processors within a node
- snoopy protocol within each node