# CS 4290

| Lectures                                                                |
| :---------------------------------------------------------------------- |
| [Introduction](#introduction)                                           |
| [Performance Metrics](#performance-metrics)                             |
| [Pipelining Review](#pipelining-review)                                 |
| [Branch Prediction and Predication](#branch-prediction-and-predication) |

---

## Introduction

### Ways to speed up a computer
1. speculation (just execute it)
2. prediction
3. parallelism

### Flynn's View of Parallelism
- SISD: single instruction, single data (single-threaded processors)
- SIMD: single instruction, multiple data (vector processors)
- MISD: multiple instruction, single data (i.e. cryptography)

### Hybrids
- SPMD - single program, multiple data (GPUs)

### Moore's Law

---

## Performance Metrics

### Choosing Metrics (Carefully)
- MIPS (million instructions per second)
- MFLOPS (million floating point operations per second)
- Peak FLOP - not representative (not always achievable)
- **run time (CPU time)** - does not include I/O

### CPU Performance
CPU time = CPU Clock Cycles * Clock cycle time = seconds per program

### RISC vs CISC

#### RISC - Simple Instructions (Reduced)
- increases instruction count (IC)
- easier to build hardware (decreases CT)
- more memory needed

#### CISC - Complex Instructions
- descreases IC
- easier to program
- harder to build fast hardware

CISC was used when memory was expensive, but now memory is cheap, and compilers are better, so RISC is more often used.

### Dealing with Numbers
- arithmetic mean to combine runtimes
- harmonic mean to combine rates
- geometric mean for normalized values 

### Amdahl's Law
speedup = old execution time / new execution time  
= 1 / ((1-fraction<sub>enhanced</sub>)+fraction<sub>enhanced</sub>/speedup<sub>enhanced</sub>))

- Make the common case faster (improve everything by a little bit, rather than highly optimizing a small part).
- diminishing returns - with each optimization of the same section, each optimization will have less effect

---

## Pipelining Review

### Typical Stages
- instruction fetch (IF)
- instruction decode (ID)
- execute (EX)
- memory (MEM)
- write-back (WB)

Most machines like this are not Von-Neuman?? machines, meaning that instructions are treated like data (they are both stored in memory).

### One instruction per cycle
- with no pipelining, the clock cycle time must be long enough for the most time consuming instruction
- with pipelining, the clock cycle time can be shortened to just the longest stage of the pipeline

### Dependencies
- RAW - true dependency

#### False Dependencies
Can be an issue in out of order processers
- WAR - anti-dependency
- WAW - 

### Register Files
- write on rising edge, and read on the falling edge (eliminates one noop from the pipeline with no data forwarding)

### Hazards
- data - dependencies prevent overlapped execution
- structural - not enough hardware resources (e.g. 3 adds in a row, but only 1 ALU - why is this an issue, since everything is being pipelined anyway??)
- control - branches (flushing)

---

## Branch Prediction and Predication

### Branch Stalls
- target address must be known one cycle after fetching branch, but the address is usually determined many slots later, even with direct branches (in the decode stage, one stage too late)

### Branch Delay Slots
- push problem to compiler, fill with instructions that are executed regardless of the branch
- too many cons (not feasible with longer pipelines)

### Branching
Execute anyway based on a guess, rather than waiting until the branch is resolved

| Type                                     | Whether                | Where       |
| ---------------------------------------- | ---------------------- | ----------- |
| Direct / Unconditional<br>Function Calls | Easy<br>(always taken) | Easy        |
| Conditional                              | Difficult              | Easy        |
| Indirect Jumps<br>Function Returns       | Easy<br>(always taken) | "Difficult" |

### Branch Target Buffer
- Holds the target of the last branch taken by the instruction at the given address
 (indexed by PC address).
- If an address matches, that branch is predicted to be taken.
- BTB entry is updated once the target is known

This is used to predict regular branches, and function calls (first two types in the table above).

### Function Returns
Use a return address stack.

#### Recursion
If the same function is called many times, the address does not continually need to be pushed (otherwise RAS will be filled, and previous addresses will be overwritten). Instead, count the number of times the same function has recursed, and only pop when reaching the end. Some extra hardware will be needed for this.

#### Full RAS
- overwrite, do nothing, or keep a counter

### Static Prediction
- always predict a certain direction

#### not taken
- easy to implement
- not so good accuracy (30-40%)

#### taken
- 60-70% accuracy

### Dynamic Prediction
- look up predictor table
- update branch history

### Use more than one bit
- create an FSM

### Branching Importance
- improving from 98% to 99% halves the number of mispredictions
- halving the miss rate doubles the number of useful instructions fetched

### Track the History of a Branch
- mix between FSM and keeping track of previous outcomes
- have a bit for past outcome, and counters for each state of the previous outcome
- n bits for last n outcomes, 2^n counters

#### Local History
- using the previously mentioned techniques
- predicting the direction of Branch A given the previous outcomes of instances of Vranch A

#### Global History
- predicted direction of a branch Z given outcomes of all previous branches (not limited to only branch Z)
- limited by the history length
- this is useful for related branch conditions (e.g. `if` statements with opposite clauses)

### Bimodal Predictor
- generate pattern history table (PHT) index by hashing the branch PC value
- each entry in the PHT is a counter for the given PC value

### G-select
- generate PHT index by concatenating PC with global history
- usually equal numbers of bits from PC and history (n/2)

### G-share
- concatenation in G-select will waste a lot of rows
- instead, generate the PHT index by xor-ing branch PC with global history

### Local History Predictor
- two layer indexing; one history register table (HRT) to record history for each PC, and a separate PHT for each history entry
- no one uses this anymore

### Two-Level Adaptive Predictor
- instead of separate PHTs for each entry, have a single global PHT
- use PC to index into HRT, and HRT entry value to index into PHT

### Loop Predictor
- previous predictors are not very useful for loops
- have a separate loop predictor
- how to determine if a branch is a loop?

### Tournament Predictor
- use a predictor to determine which predictor will be better
- "meta-predictor" will select which predictor to use for a given branch

### Updating History
- branches can occur one after another (nested ifs, for-loops, etc.)
- for deeper pipelines, waiting for the outcome of the branch (the EXECUTE stage) may take too long
- option: use predicted value to update history